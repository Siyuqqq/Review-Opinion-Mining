{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_Sentence Classification Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse & clearn labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('Restaurants_Train.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'sentences' at 0x1a1ef5f4f8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3044 reviews in this training set\n"
     ]
    }
   ],
   "source": [
    "# Use this dataframe for multilabel classification\n",
    "# Must use scikitlearn's multilabel binarizer\n",
    "\n",
    "labeled_reviews = []\n",
    "for sentence in root.findall(\"sentence\"):\n",
    "    entry = {}\n",
    "    aterms = []\n",
    "    aspects = []\n",
    "    if sentence.find(\"aspectTerms\"):\n",
    "        for aterm in sentence.find(\"aspectTerms\").findall(\"aspectTerm\"):\n",
    "            aterms.append(aterm.get(\"term\"))\n",
    "    if sentence.find(\"aspectCategories\"):\n",
    "        for aspect in sentence.find(\"aspectCategories\").findall(\"aspectCategory\"):\n",
    "            aspects.append(aspect.get(\"category\"))\n",
    "    entry[\"text\"], entry[\"terms\"], entry[\"aspects\"]= sentence[0].text, aterms, aspects\n",
    "    labeled_reviews.append(entry)\n",
    "labeled_df = pd.DataFrame(labeled_reviews)\n",
    "print(\"there are\",len(labeled_reviews),\"reviews in this training set\")\n",
    "#    print(sentence.find(\"aspectCategories\").findall(\"aspectCategory\").get(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspects</th>\n",
       "      <th>terms</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[service]</td>\n",
       "      <td>[staff]</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[food, anecdotes/miscellaneous]</td>\n",
       "      <td>[food]</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[food]</td>\n",
       "      <td>[food, kitchen, menu]</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[service]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Where Gabriela personaly greets you and recomm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[anecdotes/miscellaneous]</td>\n",
       "      <td>[]</td>\n",
       "      <td>For those that go once and don't enjoy it, all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           aspects                  terms  \\\n",
       "0                        [service]                [staff]   \n",
       "1  [food, anecdotes/miscellaneous]                 [food]   \n",
       "2                           [food]  [food, kitchen, menu]   \n",
       "3                        [service]                     []   \n",
       "4        [anecdotes/miscellaneous]                     []   \n",
       "\n",
       "                                                text  \n",
       "0               But the staff was so horrible to us.  \n",
       "1  To be completely fair, the only redeeming fact...  \n",
       "2  The food is uniformly exceptional, with a very...  \n",
       "3  Where Gabriela personaly greets you and recomm...  \n",
       "4  For those that go once and don't enjoy it, all...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save annotated reviews\n",
    "labeled_df.to_pickle(\"annotated_reviews_df.pkl\")\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with Naive Bayes\n",
    "1. replace pronouns with neural coref\n",
    "2. train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from neuralcoref import Coref\n",
    "import en_core_web_sm\n",
    "spacy = en_core_web_sm.load()\n",
    "#coref = Coref(nlp=spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function for replacing pronouns using neuralcoref\n",
    "def replace_pronouns(text):\n",
    "    coref.one_shot_coref(text)\n",
    "    return coref.get_resolved_utterances()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspects</th>\n",
       "      <th>terms</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[service]</td>\n",
       "      <td>[staff]</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[food, anecdotes/miscellaneous]</td>\n",
       "      <td>[food]</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[food]</td>\n",
       "      <td>[food, kitchen, menu]</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           aspects                  terms  \\\n",
       "0                        [service]                [staff]   \n",
       "1  [food, anecdotes/miscellaneous]                 [food]   \n",
       "2                           [food]  [food, kitchen, menu]   \n",
       "\n",
       "                                                text  \n",
       "0               But the staff was so horrible to us.  \n",
       "1  To be completely fair, the only redeeming fact...  \n",
       "2  The food is uniformly exceptional, with a very...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read annotated reviews df, which is the labeled dataset for training\n",
    "# This is located in the pickled files folder\n",
    "annotated_reviews_df = pd.read_pickle(\"annotated_reviews_df.pkl\")\n",
    "annotated_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column for text whose pronouns have been replaced\n",
    "annotated_reviews_df[\"text_pro\"] = annotated_reviews_df.text.map(lambda x: replace_pronouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment below to pickle the new df\n",
    "# annotated_reviews_df.to_pickle(\"annotated_reviews_df2.pkl\")\n",
    "\n",
    "# Read pickled file with replaced pronouns if it exists already\n",
    "annotated_reviews_df = pd.read_pickle(\"annotated_reviews_df2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert the multi-labels into arrays\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(annotated_reviews_df.aspects)\n",
    "X = annotated_reviews_df.text#_pro\n",
    "\n",
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# save the the fitted binarizer labels\n",
    "# This is important: it contains the how the multi-label was binarized, so you need to\n",
    "# load this in the next folder in order to undo the transformation for the correct labels.\n",
    "filename = 'mlb.pkl'\n",
    "pickle.dump(mlb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8625492772667542\n",
      "f1 score: 0.6767426687378034\n"
     ]
    }
   ],
   "source": [
    "## Baseline model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# LabelPowerset allows for multi-label classification\n",
    "# Build a pipeline for multinomial naive bayes classification\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                     #('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', LabelPowerset(MultinomialNB()))])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8688567674113009\n",
      "f1 score: 0.7141696449199172\n"
     ]
    }
   ],
   "source": [
    "## with tfidf transformer:\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', LabelPowerset(MultinomialNB(alpha=1e-2))),])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8696452036793693\n",
      "f1 score: 0.6920267443287555\n"
     ]
    }
   ],
   "source": [
    "# Test if SVM performs better\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-svm', LabelPowerset(\n",
    "                             SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, max_iter=10, random_state=42)))])\n",
    "_ = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted_svm == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted_svm, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8638633377135349\n",
      "f1 score: 0.684833424807028\n"
     ]
    }
   ],
   "source": [
    "## random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_clf_rf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-rf', LabelPowerset(\n",
    "                             RandomForestClassifier(n_estimators = 100)))])\n",
    "_ = text_clf_rf.fit(X_train, y_train)\n",
    "predicted_rf = text_clf_rf.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted_rf == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted_rf, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.859395532194481\n",
      "f1 score: 0.6954693598579997\n"
     ]
    }
   ],
   "source": [
    "# GBDT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "text_clf_gbdt = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-gbdt', LabelPowerset(\n",
    "                             GradientBoostingClassifier(n_estimators=200)))])\n",
    "_ = text_clf_gbdt.fit(X_train, y_train)\n",
    "\n",
    "predicted_gbdt = text_clf_gbdt.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted_gbdt == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted_gbdt, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8877792378449408\n",
      "f1 score: 0.7511756388967253\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf_lr = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-lr', LabelPowerset(\n",
    "                             LogisticRegression(C=10)))])\n",
    "_ = text_clf_lr.fit(X_train, y_train)\n",
    "predicted_lr = text_clf_lr.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "np.mean(predicted_lr == y_test)\n",
    "\n",
    "print('mean accuracy: {}'.format(np.mean(predicted_lr == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted_lr, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8825229960578187\n",
      "f1 score: 0.748642273567034\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "text_clf_mlp = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-mlp', LabelPowerset(\n",
    "                             MLPClassifier(hidden_layer_sizes=(200,100))))])\n",
    "_ = text_clf_mlp.fit(X_train, y_train)\n",
    "predicted_mlp = text_clf_mlp.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted_mlp == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted_mlp, average='weighted')  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "text_clf_mlp = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-mlp', LabelPowerset(\n",
    "                             MLPClassifier(hidden_layer_sizes=(20,10),max_iter=500)))])\n",
    "\n",
    "param_dict = {\n",
    "    \"clf-mlp__classifier__activation\": ['identity', 'logistic', 'tanh','relu']\n",
    "}\n",
    "kf = kfolds = KFold(n_splits = 4,shuffle=True)\n",
    "#clf = MLPClassifier(max_iter=5000, learning_rate=\"adaptive\")\n",
    "#gs = GridSearchCV(clf, param_dict, cv=kf)\n",
    "gs = GridSearchCV(text_clf_mlp, param_grid = param_dict, cv = kf)\n",
    "_ = gs.fit(X_train, y_train)\n",
    "\n",
    "predicted_mlp = gs.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "np.mean(predicted_mlp == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "text_clf_lr = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-lr', LabelPowerset(\n",
    "                             LogisticRegression(C=10)))])\n",
    "\n",
    "param_dict = {\n",
    "    #\"clf-lr__classifier__C\": [1e-1,1e-2,1e-3,1e-4,1,10,100],\n",
    "    #'clf-mlp__classifier__hidden_layer_sizes':[(100,100),(100,50),(100,200),(200,100),(100,50,10),(200)],\n",
    "    'clf-lr__classifier__class_weight':['balanced',None],\n",
    "    'clf-lr__classifier__solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "kf = kfolds = KFold(n_splits = 5,shuffle=True)\n",
    "#clf = MLPClassifier(max_iter=5000, learning_rate=\"adaptive\")\n",
    "#gs = GridSearchCV(clf, param_dict, cv=kf)\n",
    "gs = GridSearchCV(text_clf_lr, param_grid = param_dict, cv = kf, verbose=1)\n",
    "_ = gs.fit(X_train, y_train)\n",
    "\n",
    "predicted_lr = gs.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "np.mean(predicted_Lr == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Train naive bayes on full dataset and save model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf_lr = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1,1))),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                         ('clf-lr', LabelPowerset(\n",
    "                             LogisticRegression(C=10)))])\n",
    "text_clf.fit(X, y)\n",
    "\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'naive_model1.pkl'\n",
    "pickle.dump(text_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mlb.inverse_transform(predicted)\n",
    "pred_df = pd.DataFrame(\n",
    "    {'text_pro': X_test,\n",
    "     'pred_category': mlb.inverse_transform(predicted)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_pro</th>\n",
       "      <th>pred_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>It's better than being on the roof of Sutton Place with 19 year old interns jabbing you in the ribs all night.</td>\n",
       "      <td>(anecdotes/miscellaneous,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>Don't expect to sit down inside though, there are only a few tables and they are always full.</td>\n",
       "      <td>(ambience,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>Again, if you are in this neighborhood - by all means, come here.</td>\n",
       "      <td>(anecdotes/miscellaneous,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>Go there to relax and feel like your somewhere else.</td>\n",
       "      <td>(ambience,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>As far as the service goes, the waitresses were not particularly friendly, but they got the job done.</td>\n",
       "      <td>(service,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            text_pro  \\\n",
       "453   It's better than being on the roof of Sutton Place with 19 year old interns jabbing you in the ribs all night.   \n",
       "1611  Don't expect to sit down inside though, there are only a few tables and they are always full.                    \n",
       "2078  Again, if you are in this neighborhood - by all means, come here.                                                \n",
       "2715  Go there to relax and feel like your somewhere else.                                                             \n",
       "2602  As far as the service goes, the waitresses were not particularly friendly, but they got the job done.            \n",
       "\n",
       "                   pred_category  \n",
       "453   (anecdotes/miscellaneous,)  \n",
       "1611  (ambience,)                 \n",
       "2078  (anecdotes/miscellaneous,)  \n",
       "2715  (ambience,)                 \n",
       "2602  (service,)                  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 100000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    loaded_embeddings_ft[1,:]=np.random.randn(300)*0.01\n",
    "    words_ft = {'<pad>':0,'<unk>':1}\n",
    "    idx2words_ft = {0:'<pad>',1:'<unk>'}\n",
    "    ordered_words_ft = ['<pad>','<unk>']\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent):\n",
    "    sent = sent.replace('<br />','')\n",
    "    tokens = spacy(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "# takenize the sentence and change to lowercase\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    new_data = []\n",
    "    for sent in dataset:\n",
    "        new_data.append(tokenize(sent))\n",
    "    return new_data\n",
    "\n",
    "def token2id(data,max_length):\n",
    "    data_id = []\n",
    "    for i in range(len(data)):\n",
    "        sent_id_1 = []\n",
    "        for word in data[i]:\n",
    "            if word in words_ft:\n",
    "                sent_id_1.append(words_ft[word])\n",
    "            else:\n",
    "                word = '<unk>'\n",
    "                sent_id_1.append(words_ft[word])      \n",
    "        data_id.append(sent_id_1[:max_length])\n",
    "    return data_id\n",
    "\n",
    "def padding_embedding(data):\n",
    "    data_paded_embeded = []\n",
    "    for sent in data:\n",
    "        sent_pad = np.pad(np.array(sent), pad_width=((0,MAX_SENTENCE_LENGTH-len(sent))), mode=\"constant\", constant_values=0)\n",
    "        sent_embed = []\n",
    "        for word in sent_pad:\n",
    "            sent_embed.append(loaded_embeddings_ft[word])\n",
    "        data_paded_embeded.append([item for sublist in sent_embed for item in sublist])\n",
    "    return data_paded_embeded\n",
    "#padded_vec1 = np.pad(np.array(xtemp[4]), pad_width=((0,MAX_SENTENCE_LENGTH-len(xtemp[4]))), mode=\"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = tokenize_dataset(X_train)\n",
    "X_train = token2id(X_train,MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xtemp = X_test.head()\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "X_test = tokenize_dataset(X_test)\n",
    "X_test = token2id(X_test,MAX_SENTENCE_LENGTH)\n",
    "X_test = padding_embedding(X_test)\n",
    "\n",
    "X_train = tokenize_dataset(X_train)\n",
    "X_train = token2id(X_train,MAX_SENTENCE_LENGTH)\n",
    "X_train = padding_embedding(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.8654402102496714\n",
      "f1 score: 0.695831232447296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "text_clf = Pipeline([('clf', LabelPowerset(LogisticRegression(C=10)))])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "print('mean accuracy: {}'.format(np.mean(predicted == y_test)))\n",
    "print('f1 score: {}'.format(f1_score(y_test, predicted, average='weighted') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
